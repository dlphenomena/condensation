{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condensation Phenomenon \n",
    "The condensation phenomenon in neural networks describes how, during the nonlinear training of neural networks, neurons in the same layer tend to condense into groups with similar outputs. We will demonstrate this effect in the accompanying code example, using a fully connected network trained to fit a one-dimensional function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Papers\n",
    "[1] Tao Luo#, Zhi-Qin John Xu#, Zheng Ma, Yaoyu Zhang*, Phase diagram for two-layer ReLU neural networks at infinite-width limit. arxiv 2007.07497 (2020), Journal of Machine Learning Research (2021) [pdf](https://ins.sjtu.edu.cn/people/xuzhiqin/pub/phasediagram2020.pdf), and in [arxiv](https://arxiv.org/abs/2007.07497). \n",
    "\n",
    "[2] Hanxu Zhou, Qixuan Zhou, Tao Luo, Yaoyu Zhang*, Zhi-Qin John Xu*, Towards Understanding the Condensation of Neural Networks at Initial Training. arxiv 2105.11686 (2021) [pdf](https://ins.sjtu.edu.cn/people/xuzhiqin/pub/initial2105.11686.pdf), and in [arxiv](https://arxiv.org/abs/2105.11686), see slides and [video talk in Chinese](https://www.bilibili.com/video/BV1tb4y1d7CZ/?spm_id_from%253D333.999.0.0), NeurIPS2022. \n",
    "\n",
    "[3] Zhi-Qin John Xu*, Yaoyu Zhang, Zhangchen Zhou, An overview of condensation phenomenon in deep learning. arXiv:2504.09484 (2025), [pdf](https://ins.sjtu.edu.cn/people/xuzhiqin/pub/condensationoverview2025.pdf), and in [arxiv](https://arxiv.org/abs/2504.09484).\n",
    "\n",
    "For more details, see [xuzhiqin condense](https://ins.sjtu.edu.cn/people/xuzhiqin/pubcondense.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里我们导入一些常用的库\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import re\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condense \n",
    "![Condense](./pic/condense.png)\n",
    "This is an ideal illustration of condensation. At initialization, the input weights of each neuron differ greatly due to randomly initialization, represented by different colors. However, after a period of training, the intermediate hidden neurons are divided into two groups: the first two neurons form one group, and the last three form another. Within each group, the input weights of different neurons are exactly the same (same color), and thus their outputs are also identical."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Default configuration parameter settings.\n",
    "`argparse` is a Python package that provides a convenient way to parse command line arguments. It allows us to define the arguments our program expects and will parse them for us. This makes it easy to write user-friendly command-line interfaces for our programs.\n",
    "\n",
    "To use `argparse`, we first create an `ArgumentParser` object, which will hold all the information necessary to parse the command-line arguments. We then define the arguments we expect using the `add_argument` method. This method takes several parameters, such as the name of the argument, its type, and a help message.\n",
    "\n",
    "In this code, we are using `argparse` to parse the command-line arguments that are passed to the program. We define several arguments, such as the learning rate, optimizer, and number of epochs, and then parse them using `args, unknown = parser.parse_known_args()`. This allows us to easily customize the behavior of our program without having to modify the code itself.\n",
    "\n",
    "Please note that you should specify the path to your own directory for saving experiment results in `ini_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch 1D dataset Training')\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('--lr', default=0.00001, type=float, help='learning rate')  # 学习率\n",
    "parser.add_argument('--optimizer', default='adam', help='optimizer: sgd | adam')  # 优化器选择：sgd 或 adam\n",
    "parser.add_argument('--epochs', default=2000, type=int, metavar='N', help='number of total epochs to run')  # 总训练轮数\n",
    "parser.add_argument('--test_size', default=10000, type=int, help='the test size for model (default: 10000)')  # 测试集大小\n",
    "parser.add_argument('--save', default='trained_nets', help='path to save trained nets')  # 保存训练模型的路径\n",
    "parser.add_argument('--save_epoch', default=10, type=int, help='save every save_epochs')  # 每多少轮保存一次模型\n",
    "parser.add_argument('--rand_seed', default=0, type=int, help='seed for random num generator')  # 随机数生成器的种子\n",
    "parser.add_argument('--gamma', type=float, default=2, help='parameter initialization distribution variance power(We first assume that each layer is the same width.)')  # 参数初始化分布方差幂（假设每层宽度相同）\n",
    "parser.add_argument('--boundary', nargs='+', type=str, default=['-1', '1'], help='the boundary of 1D data')  # 一维数据的边界\n",
    "parser.add_argument('--training_size', default=80, type=int, help='the training size for model (default: 1000)')  # 训练集大小\n",
    "parser.add_argument('--act_func_name', default='Tanh', help='activation function')  # 激活函数名称\n",
    "parser.add_argument('--hidden_layers_width', nargs='+', type=int, default=[50])  # 隐藏层宽度\n",
    "parser.add_argument('--input_dim', default=5, type=int, help='the input dimension for model (default: 1)')  # 模型输入维度\n",
    "parser.add_argument('--output_dim', default=1, type=int, help='the output dimension for model (default: 1)')  # 模型输出维度\n",
    "parser.add_argument('--device', default='cuda', type=str, help='device used to train (cpu or cuda)')  # 训练使用的设备（CPU 或 CUDA）\n",
    "parser.add_argument('--plot_epoch', default=100, type=int, help='step size of plotting interval (default: 1000)')  # 绘图间隔的步长\n",
    "parser.add_argument('--ini_path', default='/home/zhouzhangchen/condensation', type=str, help='the path to save experiment results')  # 保存实验结果的路径\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Make the Directories of the Expetiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdirs(fn):  # Create directorys\n",
    "    if not os.path.isdir(fn):\n",
    "        os.makedirs(fn)\n",
    "    return fn\n",
    "\n",
    "\n",
    "def create_save_dir(path_ini): \n",
    "    subFolderName = re.sub(r'[^0-9]', '', str(datetime.datetime.now()))\n",
    "    path = os.path.join(path_ini, subFolderName)\n",
    "    mkdirs(path)\n",
    "    # mkdirs(os.path.join(path, 'output'))\n",
    "    return path\n",
    "\n",
    "\n",
    "args.path = create_save_dir(args.ini_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data for training and testing\n",
    "\n",
    "The target function is:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x)=\\sum\\limits_{k=1}^53.5\\sin(5x_k+1)\n",
    "\\end{equation}\n",
    "where the data $\\boldsymbol{x} = (x_1,x_2,x_3,x_4,x_5)$ are uniformly sampled from the boundaries are $[-4,2]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(x):  # Function to fit\n",
    "    y = 3.5 * torch.sum(torch.sin(5*x+1),dim=1)\n",
    "    y = torch.unsqueeze(y, 1)\n",
    "    return y\n",
    "\n",
    "for i in range(2):\n",
    "    if isinstance(args.boundary[i], str):\n",
    "        args.boundary[i] = eval(args.boundary[i])\n",
    "\n",
    "args.test_input = torch.rand((args.test_size, args.input_dim)) * 6 - 4\n",
    "\n",
    "args.training_input = torch.rand((args.training_size, args.input_dim)) * 6 - 4\n",
    "args.test_target = get_y(args.test_input)\n",
    "args.training_target = get_y(args.training_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define activation functions\n",
    "We define $f(x)= x * \\mathrm{tanh} (x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xtanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(xtanh,self).__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x * nn.Tanh()(x) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network model and parameter initialization.\n",
    "\n",
    "Given $\\theta\\in \\mathbb{R}^M$, the FNN function $f_{\\theta}(\\cdot)$ is defined recursively. First, we denote $f^{[0]}_{\\theta}(x)=x$ for all $x\\in\\mathbb{R}^d$. Then, for $l\\in[L-1]$, $f^{[l]}_{\\theta}$ is defined recursively as \n",
    "$f^{[l]}_{\\theta}(x)=\\sigma (W^{[l]} f^{[l-1]}_{\\theta}(x)+b^{[l]})$, where $\\sigma$ is a non-linear activation function.\n",
    "Finally, we denote\n",
    "\\begin{equation*}\n",
    "    f_{\\theta}(x)=f(x,\\theta)=f^{[L]}_{\\theta}(x)=W^{[L]} f^{[L-1]}_{\\theta}(x)+b^{[L]}.\n",
    "\\end{equation*}\n",
    "\n",
    "The parameter initialization is under the Gaussian distribution as follows,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\theta_{l} \\sim N(0, \\frac{1}{m_{l}^{\\gamma}}),\n",
    "\\end{equation*}\n",
    "where the $l$ th layer parameters of $\\theta$ is the ordered pair $\\theta_{l}=\\Big(W^{[l]},b^{[l]}\\Big),\\quad l\\in[L]$, $m_{l}$ is the width of the $l$ th layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, gamma, hidden_layers_width=[100],  input_size=20, num_classes: int = 1000, act_layer: nn.Module = nn.ReLU()):\n",
    "        super(Linear, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers_width = hidden_layers_width\n",
    "        self.gamma = gamma\n",
    "        layers: List[nn.Module] = []\n",
    "        self.layers_width = [self.input_size]+self.hidden_layers_width\n",
    "        for i in range(len(self.layers_width)-1):\n",
    "            layers += [nn.Linear(self.layers_width[i],\n",
    "                                    self.layers_width[i+1]), act_layer]\n",
    "        layers += [nn.Linear(self.layers_width[-1], num_classes, bias=False)]\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "\n",
    "        for obj in self.modules():\n",
    "            if isinstance(obj, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.normal_(obj.weight.data, 0, 1 /\n",
    "                                self.hidden_layers_width[0]**(self.gamma))\n",
    "                if obj.bias is not None:\n",
    "                    nn.init.normal_(obj.bias.data, 0, 1 /\n",
    "                                    self.hidden_layers_width[0]**(self.gamma))\n",
    "\n",
    "\n",
    "def get_act_func(act_func):\n",
    "    if act_func == 'Tanh':\n",
    "        return nn.Tanh()\n",
    "    elif act_func == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    elif act_func == 'Sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif act_func == 'xTanh':\n",
    "        return xtanh()\n",
    "    else:\n",
    "        raise NameError('No such act func!')\n",
    "\n",
    "\n",
    "act_func = get_act_func(args.act_func_name)\n",
    "\n",
    "model = Linear(args.gamma, args.hidden_layers_width, args.input_dim,\n",
    "               args.output_dim, act_func).to(args.device)\n",
    "\n",
    "para_init = copy.deepcopy(model.state_dict())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-step training function.\n",
    "\n",
    "The training data set is denoted as  $S=\\{(x_i,y_i)\\}_{i=1}^n$, where $x_i\\in\\mathbb{R}^d$ and $y_i\\in \\mathbb{R}^{d'}$. For simplicity, we assume an unknown function $y$ satisfying $y(x_i)=y_i$ for $i\\in[n]$. The empirical risk reads as\n",
    "\\begin{equation*}\n",
    "    R_S(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\ell(f(x_i,\\theta),y(x_i)),\n",
    "\\end{equation*}\n",
    "where the loss function $\\ell(\\cdot,\\cdot)$ is differentiable and the derivative of $\\ell$ with respect to its first argument is denoted by $\\nabla\\ell(y,y^*)$. \n",
    "\n",
    "For a one-step gradient descent, we have, \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\theta_{t+1}=\\theta_t-\\eta\\nabla R_S(\\theta).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, loss_fn,  args):\n",
    "\n",
    "    model.train()\n",
    "    device = args.device\n",
    "    data, target = args.training_input.to(\n",
    "        device), args.training_target.to(device).to(torch.float)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(data)\n",
    "    loss = loss_fn(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-step test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, args):\n",
    "    model.eval()\n",
    "    device = args.device\n",
    "    with torch.no_grad():\n",
    "        data, target = args.test_input.to(\n",
    "            device), args.test_target.to(device).to(torch.float)\n",
    "        outputs = model(data)\n",
    "        loss = loss_fn(outputs, target)\n",
    "\n",
    "    return loss.item(), outputs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the loss value during the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(path, loss_train, x_log=False):\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    y2 = np.asarray(loss_train)\n",
    "    plt.plot(y2, 'k-', label='Train')\n",
    "    plt.xlabel('epoch', fontsize=18)\n",
    "    ax.tick_params(labelsize=18)\n",
    "    plt.yscale('log')\n",
    "    if x_log == False:\n",
    "        fntmp = os.path.join(path, 'loss.jpg')\n",
    "\n",
    "    else:\n",
    "        plt.xscale('log')\n",
    "        fntmp = os.path.join(path, 'loss_log.jpg')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fntmp,dpi=300)\n",
    "\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the figure of the model output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_output(path, args, output, epoch):\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    plt.plot(args.training_input.detach().cpu().numpy(),\n",
    "             args.training_target.detach().cpu().numpy(), 'b*', label='True')\n",
    "    plt.plot(args.test_input.detach().cpu().numpy(),\n",
    "             output.detach().cpu().numpy(), 'r-', label='Test')\n",
    "\n",
    "    ax.tick_params(labelsize=18)\n",
    "    plt.legend(fontsize=18)\n",
    "    fn = mkdirs(os.path.join('%s'%path,'output'))\n",
    "    fntmp = os.path.join(fn, str(epoch)+'.jpg')\n",
    "\n",
    "    plt.savefig(fntmp, dpi=300)\n",
    "\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Functions to Visualize Features of High-Dimensional Neurons ($d>1$) During Training\n",
    "\n",
    "For high-dimensional data, polar coordinates cannot be used. In this case, we can utilize cosine similarity to measure the size of the angle between two vectors.\n",
    "\n",
    "**Cosine Similarity**: The cosine similarity between two vectors $\\boldsymbol{u}$ and $\\boldsymbol{v}$ is defined as\n",
    "\\begin{equation}\n",
    "D(\\boldsymbol{u},\\boldsymbol{v}) = \\frac{\\boldsymbol{u}^T\\boldsymbol{v}}{(\\boldsymbol{u}^T\\boldsymbol{u})^{1/2}(\\boldsymbol{v}^{T}\\boldsymbol{v})^{1/2}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter(checkpoint):\n",
    "    wei1 = checkpoint['features.0.weight']\n",
    "    bias = checkpoint['features.0.bias']\n",
    "    wei2 = checkpoint['features.2.weight']\n",
    "\n",
    "    return wei1, bias, wei2\n",
    "\n",
    "def normalize_vectorgroup(checkpoint):\n",
    "    wei1, bias, wei2 = get_parameter(checkpoint)\n",
    "    bias = torch.unsqueeze(bias,dim=1)\n",
    "    vector_group = torch.cat((wei1,bias),dim=1)\n",
    "    vector_group = vector_group.detach().cpu().numpy()\n",
    "    norms = np.linalg.norm(vector_group,axis=1)\n",
    "    mask = norms > 0\n",
    "    vector_masked = vector_group[mask]\n",
    "    norms = norms[mask]\n",
    "    norms = norms[:, np.newaxis]\n",
    "    vector_normalized = vector_masked / norms\n",
    "    return vector_normalized,vector_masked.shape[0]\n",
    "\n",
    "\n",
    "def seperate_vectors_by_eigenvector(vector_group):\n",
    "    mask = np.linalg.norm(vector_group,axis=1) > 0\n",
    "    vector_group = vector_group[mask]\n",
    "    similarity_matrix = np.dot(vector_group,vector_group.transpose())\n",
    "    w,v = np.linalg.eig(similarity_matrix)\n",
    "    index = np.argmax(w)\n",
    "    tmpeig = v[:,index]\n",
    "    order_mask = np.argsort(tmpeig)\n",
    "    \n",
    "    similarity_matrix = similarity_matrix[order_mask,:]\n",
    "    similarity_matrix = similarity_matrix[:,order_mask]\n",
    "    return similarity_matrix,order_mask\n",
    "\n",
    "def plot_weight_heatmap_eigen(weight, path, args, nota=''):\n",
    "\n",
    "    weight_normalized,masked_shape = normalize_vectorgroup(weight)\n",
    "    similarity_matrix,order = seperate_vectors_by_eigenvector(weight_normalized)\n",
    "    fn = mkdirs(os.path.join('%s'%path,'cosine_similarity'))\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(similarity_matrix,vmin=-1,vmax=1,cmap='YlGnBu')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('index',fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.ylabel('index',fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fn,'%s'%nota))\n",
    "    plt.close()\n",
    "    return order"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process\n",
    "With the definitions of functions related to the training process, we can now train the neural network and visualize the features of the neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.gamma = 4\n",
    "args.lr = 0.001\n",
    "args.epochs = 100\n",
    "args.save_epoch = 10\n",
    "args.plot_epoch = 10\n",
    "args.optimizer = 'adam'\n",
    "args.act_func_name = 'Tanh'\n",
    "args.savepath = os.path.join(args.path, 't=%s'%args.gamma)\n",
    "os.makedirs(args.savepath, exist_ok=True)\n",
    "act_func = get_act_func(args.act_func_name)\n",
    "\n",
    "model = Linear(args.gamma, args.hidden_layers_width, args.input_dim,\n",
    "               args.output_dim, act_func).to(args.device)\n",
    "\n",
    "para_init = copy.deepcopy(model.state_dict())\n",
    "if args.optimizer=='sgd':\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "else:\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "t0 = time.time()\n",
    "loss_training_lst=[]\n",
    "loss_test_lst = []\n",
    "for epoch in range(args.epochs+1):\n",
    "\n",
    "      model.train()\n",
    "      loss = train_one_step(\n",
    "        model, optimizer, loss_fn, args)\n",
    "      loss_test, output = test(\n",
    "          model, loss_fn, args)\n",
    "      loss_training_lst.append(loss)\n",
    "      loss_test_lst.append(loss_test)\n",
    "      if epoch % args.plot_epoch == 0:\n",
    "            print(\"[%d] loss: %.6f valloss: %.6f time: %.2f s\" %\n",
    "                  (epoch + 1, loss, loss_test, (time.time()-t0)))\n",
    "  \n",
    "      if (epoch+1) % (args.plot_epoch) == 0:\n",
    "          plot_loss(path=args.savepath,\n",
    "                    loss_train=loss_training_lst, x_log=True)\n",
    "          plot_loss(path=args.savepath,\n",
    "                    loss_train=loss_training_lst, x_log=False)\n",
    "\n",
    "          \n",
    "          para_now = copy.deepcopy(model.state_dict())\n",
    "          plot_weight_heatmap_eigen(para_now, args.savepath, args, nota='%s'%epoch)\n",
    "          \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
